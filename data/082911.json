{"archive":[{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Connectivity issues.","status":1,"date":1311085136,"service":"CloudFront","description":"<div><span class=\"yellowfg\"> 7:29 AM PDT</span>&nbsp;We are investigating connectivity issues impacting system operations in the edge location we use in St. Louis.</div><div><span class=\"yellowfg\"> 7:52 AM PDT</span>&nbsp;Between 6:30 AM PDT and 7:15 AM PDT some customers experienced connectivity issues when requesting content from the edge location we use in St. Louis. We've mitigated the issue and the service is now operating normally.</div>","details":""},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Slow Propagation","status":1,"date":1311157773,"service":"Route53","description":"<div><span class=\"yellowfg\"> 3:31 AM PDT</span>&nbsp;We experienced slow propagation of DNS edits to the Route 53 DNS servers from 02:27 PDT to 03:20 PDT. The problem is now resolved.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Instances unavailable in a single availability zone","status":1,"date":1311255072,"service":"EC2","description":"<div><span class=\"yellowfg\"> 6:32 AM PDT</span>&nbsp;We are investigating unavailability for a small number of instances in a single availability zone in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 7:05 AM PDT</span>&nbsp;We can confirm a power issue affecting some instances in a single availability zone in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 7:17 AM PDT</span>&nbsp;We can confirm a power issue affecting some instances in a single availability zone in the US-EAST-1 region. You can use the DescribeAvailabilityZone API to find out if you have instances in the affected availability zone. The state of the affected availability zone will be listed as impaired.\r\n\r\n</div><div><span class=\"yellowfg\"> 7:59 AM PDT</span>&nbsp;We are in the process of restoring power to affected instances in a single availability zone in the US-EAST-1 region. You can use the DescribeAvailabilityZone API to find out if you have instances in the affected availability zone. The state of the affected availability zone will be listed as impaired.</div><div><span class=\"yellowfg\"> 8:43 AM PDT</span>&nbsp;We have restored power to the majority of affected instances in a single availability zone in the US-EAST-1 region. We are continuing to work to restore power to a small number of instances that are still affected. You can use the DescribeAvailabilityZone API to find out if you have instances in the affected availability zone. The state of the affected availability zone will be listed as \"impaired\".</div><div><span class=\"yellowfg\"> 9:17 AM PDT</span>&nbsp;We have restored power to all affected instances in a single availability zone in the US-EAST-1 region. The service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic MapReduce (N. Virginia)","summary":"[RESOLVED] Issue affecting some job flows in a single availability zone in the US-EAST-1 region","status":1,"date":1311258124,"service":"ElasticMapReduce","description":"<div><span class=\"yellowfg\"> 7:23 AM PDT</span>&nbsp;We can confirm a power issue affecting some job flows in a single availability zone in the US-EAST-1 region</div><div><span class=\"yellowfg\"> 8:00 AM PDT</span>&nbsp;Affected job flows have recovered and the service is operating normally.</div>","details":""},{"service_name":"Amazon Simple Email Service (N. Virginia)","summary":"[RESOLVED] SES deliverability problems","status":2,"date":1311350884,"service":"SES","description":"<div><span class=\"yellowfg\"> 9:08 AM PDT</span>&nbsp;We are currently experiencing deliverability issues with SES.</div><div><span class=\"yellowfg\"> 9:40 AM PDT</span>&nbsp;We can confirm increased deliverability problems and continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:55 AM PDT</span>&nbsp;We have corrected the source of the deliverability problems and are working toward resolution.</div><div><span class=\"yellowfg\">10:36 AM PDT</span>&nbsp;We have corrected the source of the deliverability problems on the SES side.  We are working within our delivery pipeline to resolve the deliverability problems.</div><div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;We have corrected the source of the deliverability problems on the SES side.  We are continuing to work with the recipient community to resolve the deliverability problems.</div><div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We have corrected the source of the deliverability problems on the SES side.  We have worked with the receiver community to acknowledge the correction.  Our customers should expect higher than normal delivery latency for the next 2-3 hours. </div><div><span class=\"yellowfg\"> 2:15 PM PDT</span>&nbsp;We have corrected the source and receiver deliverability problems.  The service is operating normally.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (Ireland)","summary":"[COMPLETED] Maintenance on VPN Gateways and VPN Connections","status":1,"date":1311708561,"service":"VirtualPrivateCloudEU","description":"<div><span class=\"yellowfg\">12:30 PM PDT</span>&nbsp;Between 6:00 PM PDT and 10:00 PM PDT we will be conducting maintenance on VPN Gateways and VPN Connections in the eu-west-1 region. Each VPN Connection is provisioned as a pair of redundant tunnels. During this maintenance each tunnel will be unavailable for a period of time. One tunnel will be available at all times, and network connectivity to VPCs will not be affected as long as both tunnels are configured on your customer gateway.</div><div><span class=\"yellowfg\"> 8:40 PM PDT</span>&nbsp;The maintenance has been completed and the service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. California)","summary":"[RESOLVED] Elevated API error rates in the US-WEST-1 region","status":1,"date":1311752111,"service":"EC2NoCal","description":"<div><span class=\"yellowfg\">12:37 AM PDT</span>&nbsp;We are currently experiencing elevated API error rates for the CreateTags, DescribeTags and DescribeInstances APIs in the US-WEST-1 region</div><div><span class=\"yellowfg\">12:55 AM PDT</span>&nbsp;Between 11:56 PM and 12:40 AM PDT, the CreateTags, DescribeTags and DescribeInstances APIs experienced increased error rates in the US-WEST-1 region. The system is now operating normally</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (N. Virginia)","summary":"[RESOLVED] Maintenance on VPN Gateways and VPN Connections","status":1,"date":1311778470,"service":"VirtualPrivateCloud","description":"<div><span class=\"yellowfg\"> 7:58 AM PDT</span>&nbsp;Between 7:00 PM PDT and 10:00 PM PDT we will be conducting maintenance on VPN Gateways and VPN Connections in the US-EAST-1 region. Each VPN Connection is provisioned as a pair of redundant tunnels. During this maintenance each tunnel will be unavailable for a period of time. One tunnel will be available at all times, and network connectivity to VPCs will not be affected as long as both tunnels are configured on your customer gateway.</div><div><span class=\"yellowfg\">10:13 PM PDT</span>&nbsp;The maintenance has been concluded, the service is operating normally.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (Ireland)","summary":"[RESOLVED] Maintenance on VPN Gateways and VPN Connections","status":1,"date":1311890180,"service":"VirtualPrivateCloudEU","description":"<div><span class=\"yellowfg\"> 2:57 PM PDT</span>&nbsp;Between 6:00 PM PDT and 10:00 PM PDT on July 29th, we will be conducting maintenance on VPN Gateways and VPN Connections in the EU-WEST-1 region. Each VPN Connection is provisioned as a pair of redundant tunnels. During this maintenance each tunnel will be unavailable for a period of time. One tunnel will be available at all times, and network connectivity to VPCs will not be affected as long as both tunnels are configured for automatic failover on your customer gateway.</div><div><span class=\"yellowfg\">Jul 29,  9:15 PM PDT</span>&nbsp;The maintenance has been completed. The service is now operating normally.</div>","details":""},{"service_name":"AWS Elastic Beanstalk","summary":"[RESOLVED] Increased API Errors and Latencies","status":1,"date":1311898467,"service":"ElasticBeanstalk","description":"<div><span class=\"yellowfg\"> 5:15 PM PDT</span>&nbsp;We are investigating increased API errors and latencies in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 6:23 PM PDT</span>&nbsp;Service API errors and latencies have recovered. We have processed the backlog of environment creation and deletion operations. Existing environments are operating normally. We are currently investigating why normally operating environments, launched during the last hour are miss-marked gray instead of green.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;We can confirm an issue affecting our monitoring job that is reporting the wrong health for newly created environments. The console continues to report gray status for these environments. The environments are operating normally.</div><div><span class=\"yellowfg\"> 8:35 PM PDT</span>&nbsp;The affected monitoring job has recovered and environment state is now reported correctly. The service is operating normally.</div>","details":""},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Slow Propagation Times","status":1,"date":1311922827,"service":"Route53","description":"<div><span class=\"yellowfg\">12:02 AM PDT</span>&nbsp;We are investigating slow propagation of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\">12:43 AM PDT</span>&nbsp;Change propagation is back to normal. From approximately 11:02 PM PDT to 12:25 AM PDT changes were delayed. Queries were not impacted during this time.</div>","details":""},{"service_name":"Amazon Simple Storage Service (US Standard)","summary":"[RESOLVED] Elevated error rates","status":1,"date":1311922576,"service":"S3US","description":"<div><span class=\"yellowfg\">Jul 29, 12:10 AM PDT</span>&nbsp;From 11:02 PM PDT to 11:06 PM PDT and from 11:13 PM PDT to 11:17 PM PDT customers may have experienced increased error rates in our East Coast facilities.  The service has recovered and is operating normally.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (N. Virginia)","summary":"[RESOLVED] VPN Connectivity Issue","status":1,"date":1311922933,"service":"VirtualPrivateCloud","description":"<div><span class=\"yellowfg\">12:13 AM PDT</span>&nbsp;Between 11:02 PM and 11:17 PM PDT, some VPN Connections in the US-EAST-1 region experienced elevated packet loss via one or both IPSec tunnels. The issue has been resolved and the service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Network Connectivity","status":1,"date":1311923588,"service":"EC2","description":"<div><span class=\"yellowfg\">12:16 AM PDT</span>&nbsp;Between 11:02 PM PDT and 11:20 PM PDT some instances in the US-EAST-1 region experienced periods of elevated packet loss affecting connectivity within the region, and between EC2 and S3. The issue has been resolved and the service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. California)","summary":"[RESOLVED] DNS errors","status":1,"date":1312263051,"service":"EC2NoCal","description":"<div><span class=\"yellowfg\">10:31 PM PDT</span>&nbsp;We can confirm an issue affecting DNS requests originating from a small number \r\nof instances in the US-West-1 Region and we are working toward a resolution.</div><div><span class=\"yellowfg\">11:10 PM PDT</span>&nbsp;The DNS issue has been resolved and the service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Small number of instances unavailable in a single availability zone","status":1,"date":1312375130,"service":"EC2","description":"<div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are investigating unavailability for a small number of instances in a single availability zone in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:09 AM PDT</span>&nbsp;Between 4:55AM PDT and 5:25AM PDT a small number of instances in the US-EAST-1 region experienced periods of elevated packet loss affecting connectivity within the region. The issue has been resolved and the service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Elevated API error rates in the EU-WEST-1 region","status":1,"date":1312419806,"service":"EC2EU","description":"<div><span class=\"yellowfg\"> 5:09 PM PDT</span>&nbsp;We are currently experiencing elevated API error rates for the CreateTags, DescribeTags, DeleteTags and DescribeInstances APIs in the EU-WEST-1 region</div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;Between 04:51 PM and 05:12 PM PDT, the CreateTags, DescribeTags,\r\nDeleteTags and DescribeInstances APIs experienced increased error rates in the EU-WEST-1 region. The service is now operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Connectivity between EC2 instances and S3","status":1,"date":1312550653,"service":"EC2","description":"<div><span class=\"yellowfg\"> 6:26 AM PDT</span>&nbsp;Between 5:30AM and 5:42AM PDT, a small number of connections to S3 from EC2 instances in the US-EAST-1 region failed.  This has been resolved and the service is now operating normally.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (Ireland)","summary":"[RESOLVED] VPN Connection Status","status":1,"date":1312564153,"service":"VirtualPrivateCloudEU","description":"<div><span class=\"yellowfg\">10:14 AM PDT</span>&nbsp;Between August 3rd 11:00 PM PDT and August 4th 7:15 AM PDT, the status of VPN Connections in the EU-WEST-1 region was being incorrectly reported. This issue has been resolved and the service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"Connectivity issues","status":2,"date":1312740750,"service":"EC2EU","description":"<div><span class=\"yellowfg\">11:13 AM PDT</span>&nbsp;We are investigating connectivity issues in the EU-WEST-1 region.</div><div><span class=\"yellowfg\">11:27 AM PDT</span>&nbsp;EC2 APIs in the EU-WEST-1 region are currently impaired. We are working to restore full service. We are also investigating instance connectivity that we believe to be limited to a single Availability Zone.</div><div><span class=\"yellowfg\">11:51 AM PDT</span>&nbsp;To find out if you have instances in the affected availability zone log into the AWS Console at https://console.aws.amazon.com, navigate to the EC2 tab and click on the \"EC2 Dashboard\" link at the top of the navigation bar. The affected availability zone will be shown under the Availability Zone Status.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;The issues with the affected Availability Zone are the result of a power failure in that zone.  We are currently recovering power and anticipate that instances in the effected available zone will start to recover within the next 30-60 minutes.</div><div><span class=\"yellowfg\">12:33 PM PDT</span>&nbsp;Please note, Availability Zone designations are different for each customer.  This impact is limited to a single Availability Zone.  Please log onto the AWS Console to see your account's designated name of the impacted zone.</div><div><span class=\"yellowfg\">12:49 PM PDT</span>&nbsp;We are still working to restore power to the affected zone. We are first working on restoring power to the EC2 network for this zone, at which point we can begin recovering instances.</div><div><span class=\"yellowfg\"> 1:29 PM PDT</span>&nbsp;Some of our network devices have regained power, but we are having a problem with a generator which is preventing us from getting the affected zone back online.  We do not currently have an ETA for recovery of the affected Availability Zone.</div><div><span class=\"yellowfg\"> 1:56 PM PDT</span>&nbsp;Power to the majority of network devices has been restored.  We are now focusing on bringing EC2 instances back online.</div><div><span class=\"yellowfg\"> 2:11 PM PDT</span>&nbsp;Some instances in the affected zone have started to come back online.  We are continuing to work on restoring the remaining instances.</div><div><span class=\"yellowfg\"> 2:46 PM PDT</span>&nbsp;We have restored power and connectivity to approximately 50% of the affected instances.  We continue working to restore the remaining instances.</div><div><span class=\"yellowfg\"> 3:01 PM PDT</span>&nbsp;A quick update on what we know so far about the event.  What we have is preliminary, but we want to share it with you.  We understand at this point that a lighting strike hit a transformer from a utility provider to one of our Availability Zones in Dublin,  sparking an explosion and fire.  Normally, upon dropping the utility power provided by the transformer, electrical load would be seamlessly picked up by backup generators.  The transient electric deviation caused by the explosion was large enough that it propagated to a portion of the phase control system that synchronizes the backup generator plant, disabling some of them.   Power sources must be phase-synchronized before they can be brought online to load.  Bringing these generators online required manual synchronization. We've now restored power to the Availability Zone and are bringing EC2 instances up.  We'll be carefully reviewing the isolation that exists between the control system and other components.  The event began at 10:41 AM PDT with instances beginning to recover at 1:47 PM PDT.</div><div><span class=\"yellowfg\"> 3:55 PM PDT</span>&nbsp;We are continuing to recover impacted instances.  Note that EBS volumes in the affected zone also lost power.  Some of these volumes and EBS backed instances are taking longer to bring back online.</div><div><span class=\"yellowfg\"> 5:06 PM PDT</span>&nbsp;We are continuing to recover the remaining affected instances.  It may be several hours until all remaining instances and volumes can be recovered but we don't have a firm timeline and it may be longer to bring everything online.   We recommend re-launching your instance in a different availability zone in order to get back up and running more quickly.</div><div><span class=\"yellowfg\"> 6:25 PM PDT</span>&nbsp;We have started to see some of the EBS volumes in the affected zone recover.  We are working to restore connectivity to the remaining instances and volumes.</div><div><span class=\"yellowfg\"> 7:37 PM PDT</span>&nbsp;We are seeing slower than expected progress on recovering the remaining instances, but can now report that 60% of the impacted instances have recovered and are available.  Stopping and starting impaired instances will not help you recover your instance.  For those looking for what you can do to recover more quickly, we recommend re-launching your instance in another Availability Zone.</div><div><span class=\"yellowfg\"> 8:40 PM PDT</span>&nbsp;We continue to see slow but steady progress in recovering affected instances, with 65% of affected instances recovered and available.  Bringing additional EBS volumes back online is happening more slowly.  We will continue to update you with progress and additional information as we have it.</div><div><span class=\"yellowfg\"> 9:36 PM PDT</span>&nbsp;We have now recovered 75% of the impacted instances.</div><div><span class=\"yellowfg\">11:04 PM PDT</span>&nbsp;We know many of you are anxiously waiting for your instances and volumes to become available and we want to give you more detail on why the recovery of the remaining instances and volumes is taking so long.  Due to the scale of the power disruption, a large number of EBS servers lost power and require manual operations before volumes can be restored. Restoring these volumes requires that we make an extra copy of all data, which has consumed most spare capacity and slowed our recovery process. We've been able to restore EC2 instances without attached EBS volumes, as well as some EC2 instances with attached EBS volumes. We are in the process of installing additional capacity in order to support this process both by adding available capacity currently onsite and by moving capacity from other availability zones to the affected zone. While many volumes will be restored over the next several hours, we anticipate that it will take 24-48 hours until the process is completed. In some cases EC2 instances or EBS servers lost power before writes to their volumes were completely consistent. Because of this, in some cases we will provide customers with a recovery snapshot instead of restoring their volume so they can validate the health of their volumes before returning them to service. We will contact those customers with information about their recovery snapshot.</div>","details":""},{"service_name":"Amazon CloudWatch (Ireland)","summary":"Delayed CloudWatch metrics and alarms  for EU-WEST-1","status":2,"date":1312741129,"service":"CloudWatchEUWest","description":"<div><span class=\"yellowfg\">11:22 AM PDT</span>&nbsp;We are investigating delays in CloudWatch metrics and alarms in the EU-WEST-1 region.</div><div><span class=\"yellowfg\">12:00 PM PDT</span>&nbsp;We have increased latency for CloudWatch metrics in the EU-WEST-1 region. If a CloudWatch alarm is set on a CloudWatch delayed metric, customers may see alarms transition into an \"Insufficient Data\" state.</div><div><span class=\"yellowfg\"> 1:35 PM PDT</span>&nbsp;We continue to see metrics delayed for services operating in the Availability Zone affected by the power failure in the EU-WEST-1 region. CloudWatch is operating normally for other Availability Zones.</div><div><span class=\"yellowfg\"> 2:47 PM PDT</span>&nbsp;CloudWatch metrics are still delayed in the Availability Zone affected by the power failure. The CloudWatch service is operating normally in all other Availability Zones.</div><div><span class=\"yellowfg\"> 4:03 PM PDT</span>&nbsp;CloudWatch metrics are operating normally for all recovered instances in the affected Availability Zone. Metrics from instances still recovering from the power loss in the affected Availability Zone are still delayed.</div>","details":""},{"service_name":"Amazon Relational Database Service (Ireland)","summary":"Connectivity issues to RDS instances","status":2,"date":1312741351,"service":"RelationalDBServiceEU","description":"<div><span class=\"yellowfg\">11:26 AM PDT</span>&nbsp;We are investigating instance connectivity issues to RDS database instance in single availability zone in EU-WEST-1 region.</div><div><span class=\"yellowfg\">12:07 PM PDT</span>&nbsp;RDS team is currently investigating elevated times to complete provisioning for Create, Restore and Modify API requests. We also continue to work on the connectivity issues that is impacting active RDS database instances in single availability zone. We are working to restore full service.</div><div><span class=\"yellowfg\"> 1:37 PM PDT</span>&nbsp;We continue to work on improving the provisioning times for  Create, Restore and Modify API requests and connectivity issues with the impacted RDS database instances.</div><div><span class=\"yellowfg\"> 2:16 PM PDT</span>&nbsp;Provisioning times for  Create, Restore and Modify API requests have recovered for untargeted requests. We continue to work on restoring connectivity to the impacted RDS database instances.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We continue to make progress in restoring connectivity to the impacted RDS database instances.</div><div><span class=\"yellowfg\"> 4:31 PM PDT</span>&nbsp;We continue to restore connectivity to the impacted RDS database instances.</div><div><span class=\"yellowfg\"> 6:00 PM PDT</span>&nbsp;We continue to make progress in restoring connectivity to the impacted RDS database instances, however it may take several hours to restore connectivity to all of the remaining RDS database instances. We recommend restoring from your latest available backup to another Availability Zone to get up and running more quickly. The majority of databases using RDS with the Multi-AZ option successfully failed over to another availability zone and are operating normally. However, a small number of Multi-AZ instances did not failover successfully due to an issue in the failover control software.</div><div><span class=\"yellowfg\"> 7:58 PM PDT</span>&nbsp;We continue to make progress in restoring connectivity to the impacted RDS database instances and manually failing over the small number of Multi-AZ RDS database instances that did not automatically failover. \r\nAs suggested in the previous post, we recommend restoring from your latest available backup to another availability zone to get up and running more quickly. </div><div><span class=\"yellowfg\"> 9:16 PM PDT</span>&nbsp;We have now restored connectivity to most of the Multi-AZ RDS database instances and continue to make steady progress with restoring connectivity to Single-AZ RDS database instances. We will keep you posted as we have more information to share.</div><div><span class=\"yellowfg\">Aug  8, 12:18 AM PDT</span>&nbsp;We have now restored connectivity to all of the Multi-AZ RDS database instances. We continue to make slow progress restoring connectivity to Single-AZ RDS database instances. The rest of the Single-AZ restores are tied to recovery of EBS volumes associated with these RDS database instances.  We know many of you are waiting for your database instances to be restored.  As we explained in the earlier EC2 post, we expect to make faster progress as were able to bring more EBS capacity online.</div>","details":""},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Slow Propagation Times","status":1,"date":1312741670,"service":"Route53","description":"<div><span class=\"yellowfg\">11:29 AM PDT</span>&nbsp;We are investigating slow propagation of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=\"yellowfg\">12:51 PM PDT</span>&nbsp;Change propagation is back to normal. From approximately 10:48AM to 12:37PM PDT changes were delayed to a single edge location. Queries were not impacted during this time.</div>","details":""},{"service_name":"AWS CloudFormation (Ireland)","summary":"[RESOLVED] Delays provisioning stacks in EU-WEST-1 region","status":3,"date":1312741746,"service":"CloudFormationEUWest","description":"<div><span class=\"yellowfg\">11:47 AM PDT</span>&nbsp;We are currently investigating delays in provisioning CloudFormation stacks in the EU-West-1 region.</div><div><span class=\"yellowfg\">12:50 PM PDT</span>&nbsp;Describe calls giving visibility into existing stacks continue to function, however, we are currently unable to provision new CloudFormation stacks in the EU-West-1 region. We are continuing to work on the issue.</div><div><span class=\"yellowfg\"> 1:18 PM PDT</span>&nbsp;CloudFormation is now creating new stacks and the backlog of stack creations is draining.</div><div><span class=\"yellowfg\"> 2:27 PM PDT</span>&nbsp;The backlog in creating new CloudFormation stacks has been cleared.  The service is operating normally.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (Ireland)","summary":"[RESOLVED] VPN connectivity issues","status":1,"date":1312744056,"service":"VirtualPrivateCloudEU","description":"<div><span class=\"yellowfg\">12:09 PM PDT</span>&nbsp;We are currently investigating connectivity issues to one of our redundant VPN Gateways in the EU-WEST-1 region. Each VPN Connection is provisioned as a pair of redundant VPN tunnels to provide customers with the ability to configure a VPN tunnel to two VPN Gateway endpoints. The redundant VPN Gateway is not affected and VPN Connections with both tunnels configured are operating normally.</div><div><span class=\"yellowfg\"> 1:11 PM PDT</span>&nbsp;We can confirm that between 10:47 AM PDT and 11:34 AM PDT, one of our VPN Gateway devices in the EU-WEST-1 region was not available. Customers with both tunnels configured on their customer gateway devices were not affected. The issue with the VPN Gateway device has been resolved, and both tunnels for preexisting VPN Connections are now available. At this time, we are investigating increased latencies for Create, Terminate, and Attach operations for Virtual Gateways and VPN Connections.</div><div><span class=\"yellowfg\"> 2:14 PM PDT</span>&nbsp;Latencies for Create, Terminate, and Attach operations for Virtual Gateways and VPN Connections have returned to normal. The service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"Connectivity issues","status":2,"date":1312787325,"service":"EC2EU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n<div><span class=\"yellowfg\">Aug 7 11:04 PM PDT</span>&nbsp;We know many of you are anxiously waiting for your instances and volumes to become available and we want to give you more detail on why the recovery of the remaining instances and volumes is taking so long.  Due to the scale of the power disruption, a large number of EBS servers lost power and require manual operations before volumes can be restored. Restoring these volumes requires that we make an extra copy of all data, which has consumed most spare capacity and slowed our recovery process. We've been able to restore EC2 instances without attached EBS volumes, as well as some EC2 instances with attached EBS volumes. We are in the process of installing additional capacity in order to support this process both by adding available capacity currently onsite and by moving capacity from other availability zones to the affected zone. While many volumes will be restored over the next several hours, we anticipate that it will take 24-48 hours until the process is completed. In some cases EC2 instances or EBS servers lost power before writes to their volumes were completely consistent. Because of this, in some cases we will provide customers with a recovery snapshot instead of restoring their volume so they can validate the health of their volumes before returning them to service. We will contact those customers with information about their recovery snapshot.</div><div><span class=\"yellowfg\">Aug 8 12:40 AM PDT</span>&nbsp;We are working on installing additional EBS capacity now and expect to have it online and available soon.</div><div><span class=\"yellowfg\"> 2:26 AM PDT</span>&nbsp;We have brought additional EBS capacity online in the impacted zone, and we have again started restoring EBS volumes and EBS backed EC2 instances.  We're continuing to bring additional capacity online.  We will continue to post updates as we have new information.</div><div><span class=\"yellowfg\"> 3:48 AM PDT</span>&nbsp;We would like to provide you with an update including specific instructions on how to launch successfully in the non-impacted availability zones. Availability zone names are logical, and may be assigned different mappings per AWS customer account. To determine which AZ is impacted, login to the AWS Management Console.<br><br>\r\n\r\nIf your EBS volumes are available or in-use, you can make a snapshot of them which can be used to create a new volume in one of the non-impacted availability zones. \r\n<br><br>\r\nOnce you have the snapshot of the volumes of interest:<br>\r\n1) Create an EBS volume from the snapshot in a different Availability Zone.<br>\r\n2) Launch an instance from the same AMI in the Availability Zone that you have created the new EBS volume.<br>\r\n3) Stop the newly launched instance, and detach the EBS volumes.<br>\r\n4) Attach the new EBS volumes (created from your snapshot) to the instance.<br>\r\n5) Start the instance.<br>\r\n<br>\r\nIf you're using Elastic IPs on the previous instance, you can disassociate the Elastic IP and map it to the newly created instance as well.</div><div><span class=\"yellowfg\"> 5:54 AM PDT</span>&nbsp;Our efforts to bring additional EBS capacity online continue and were making progress restoring volumes and EBS backed instances. We'll continue to post here as more information is available.</div><div><span class=\"yellowfg\"> 9:58 AM PDT</span>&nbsp;We have added capacity to EBS and are continuing to make progress in recovering EBS volumes and EC2 instances.  We have now also enabled the ability to launch new instance-store backed EC2 instances in the impacted Availability Zone.  Customers can still not create new EBS volumes or launch new EBS backed instances in the affected zone, but we anticipate enabling that soon.  All functionality remains available in the other Availability Zones.</div><div><span class=\"yellowfg\">12:32 PM PDT</span>&nbsp;We have now enabled the ability to create new EBS backed instances and EBS volumes in the affected zone.  API functionality is now fully restored to the affected Availability Zone.  We remain focused on recovering the remaining instances and volumes and are continuing to make progress.</div><div><span class=\"yellowfg\"> 3:11 PM PDT</span>&nbsp;Separately, and independent from the power issue in the affected availability zone, we've discovered an error in the EBS software that cleans up unused snapshots.  During a recent run of this EBS software in the EU-West Region, one or more blocks in a number of EBS snapshots were incorrectly deleted. The root cause was a software error that caused the snapshot references to a subset of blocks to be missed during the reference counting process. This process compares the blocks scheduled for deletion to the blocks referenced in customer snapshots. As a result of the software error, the EBS snapshot management system in the EU-West Region incorrectly thought some of the blocks were no longer being used and deleted them.  We've addressed the error in the EBS snapshot system to prevent it from recurring.  We have now also disabled all of the snapshots that contain these missing blocks.  \r\n<br><br>\r\nWe are in the process of creating a copy of the affected snapshots where we've replaced the missing blocks with empty block(s). Customers can then create a volume from that copy and run a recovery tool on it (e.g. a file system recovery tool like fsck); in some cases this may restore normal volume operation.  We will email affected customers as soon as we have the copy of their snapshot available. You can tell if you have a snapshot that has been affected via the DescribeSnapshots API or via the AWS Management Console.  The status for the snapshot will be shown as \"error.\"  Alternately, if you have any older or more recent snapshots that were unaffected, you will be able to create a volume from those snapshots without error.  We apologize for any potential impact it might have on customers applications.</div><div><span class=\"yellowfg\"> 4:26 PM PDT</span>&nbsp;The process of creating copies of data from the affected snapshots is now complete.  These snapshots can be identified via the Description field which you can see on the AWS console or via a DescribeSnapshot API call.  The Description field contains \"Recovery Snapshot snap-xxxx\" where snap-xxx is the id of the affected snapshot.  Customers can create a volume from that copy and run a recovery tool on it (e.g. a file system recovery tool like fsck); in some cases this may restore normal volume operation.  Alternately, if you have any older or more recent snapshots that were unaffected, you will be able to create a volume from those snapshots without error.</div><div><span class=\"yellowfg\">10:01 PM PDT</span>&nbsp;We have now recovered all of the volumes and instances that we could verify were fully consistent at the time of the power outage.  For the remaining EBS volumes, we were unable to verify whether or not there were any in-flight writes that did not get consistently saved.   As a result, we've now started the process of creating recovery snapshots for all of these EBS volumes that are still unavailable.   As these recovery snapshots become available, we will put them in your account.  This process is time consuming and we expect these recovery snapshots to start to show up in customers' accounts in the next 6-8 hours, but the process might take up to 24 hours to fully complete.  We expect that a large portion of these volumes created from these recovery snapshots will be consistent, but customers will need to verify volume consistency by running a recovery tool on their new volume (e.g. a file system recovery tool like fsck).  Volumes that are consistent should be usable by all applications.  If your volume is inconsistent, your application's ability to use the volume will depend on how it handles the inconsistency.  When we make meaningful progress on delivering recovery snapshots to affected customers, we will post an update here.</div>","details":""},{"service_name":"Amazon Relational Database Service (Ireland)","summary":"Connectivity issues to RDS instances","status":2,"date":1312792675,"service":"RelationalDBServiceEU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n<div><span class=\"yellowfg\">Aug  8, 12:18 AM PDT</span>&nbsp;We have now restored connectivity to all of the Multi-AZ RDS database instances. We continue to make slow progress restoring connectivity to Single-AZ RDS database instances. The rest of the Single-AZ restores are tied to recovery of EBS volumes associated with these RDS database instances.  We know many of you are waiting for your database instances to be restored.  As we explained in the earlier EC2 post, we expect to make faster progress as were able to bring more EBS capacity online.</div><div><span class=\"yellowfg\"> 2:45 AM PDT</span>&nbsp;Multi-AZ RDS databases instances continue to operate normally. We continue to make slow progress restoring connectivity to Single-AZ RDS database instances. As we said in our earlier posts, the rest of the Single-AZ restores are tied to recovery of EBS volumes associated with these RDS database instances. We will keep you posted as we have more information to share.</div><div><span class=\"yellowfg\"> 5:55 AM PDT</span>&nbsp;As additional EBS capacity is brought online, we continue to make progress in restoring affected RDS database instances.</div><div><span class=\"yellowfg\"> 8:55 AM PDT</span>&nbsp;We continue to make progress on restoring connectivity to affected Single-AZ database instances.   As a reminder, customers with automated backups turned on for an affected database instance have the option of initiating a Point-in-Time Restore operation.  This will launch a new database instance using a backup of the affected database instance from before the event.  To do this, follow these steps:\r\n<br><br>\r\n1) Log into the AWS Management console<br>\r\n2) Access the RDS tab, and select \"DB Instances\" on the left-side navigation<br>\r\n3) Select the affected database instance<br>\r\n4) Click on the \"Restore to Point in Time\" button<br>\r\n5) Select \"Use Latest Restorable Time\"<br>\r\n6) Select a DB instance class that is at least the same size as the original DB instance<br>\r\n7) Make sure \"No Preference\" is selected for Availability Zone<br>\r\n8) Launch DB Instance and connect your application</div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;We have now re-enabled the ability for customers to create DB Instances in the affected Availability Zone within the EU-West Region.   We encourage customers with inaccessible database instances in the affected zone to initiate Point-in-Time Restore operations per the instructions posted above.</div><div><span class=\"yellowfg\">10:11 PM PDT</span>&nbsp;We have now restored access to all of the single-AZ database instances with storage volumes that we can verify were fully consistent at the time of the power outage.  The small portion of remaining single-AZ databases that are still not available have storage volumes that are inconsistent and will be moved into the \"failed storage\" state.   Our records show that the majority of impacted customers have already initiated Point-in-Time Restore operations to resume database requests.  However, if your database instance is in the \"failed storage\" state and you have not already initiated a Point-in-Time-Restore from an automated backup, we encourage you to do so immediately via the instructions posted earlier.\r\n<br><br>\r\nAdditionally, we would like to update customers on the RDS impact of the EBS snapshot issue described in an earlier EC2 status update.  A portion of user-initiated DB snapshots in the EU-West Region were impacted by the deletion of EBS snapshot blocks.   We are in the process of putting these snapshots into a \"failed\" state.   They will then be identifiable via the DescribeSnapshots API or AWS Management Console, and affected customers will be notified via email.  While these snapshots cannot be recovered, if you have any older or more recent snapshots that were unaffected, you will be able to restore from those snapshots without error.  Please also note that the \"failed\" state only applies to affected user-initiated DB snapshots.  Customers with automated backups turned on should be able to complete Point-in-time-Restore operations as normal.  \r\n<br><br>\r\nWe apologize for any potential impact this issue may have on our customers' applications.</div>","details":""},{"service_name":"Amazon CloudWatch (Ireland)","summary":"Delayed CloudWatch metrics and alarms for EU-WEST-1","status":1,"date":1312824070,"service":"CloudWatchEUWest","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n<div><span class=\"yellowfg\">Aug 7 4:03 PM PDT</span>&nbsp;CloudWatch metrics are operating normally for all recovered instances in the affected Availability Zone. Metrics from instances still recovering from the power loss in the affected Availability Zone are still delayed.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Network connectivity","status":3,"date":1312857274,"service":"EC2","description":"<div><span class=\"yellowfg\"> 7:39 PM PDT</span>&nbsp;We are investigating connectivity issues for EC2 in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 7:50 PM PDT</span>&nbsp;We can verify connectivity issues between instances in the US-EAST-1 region and the Internet.</div><div><span class=\"yellowfg\"> 8:03 PM PDT</span>&nbsp;Full connectivity has been restored.  The service is operating normally.</div>","details":""},{"service_name":"Amazon Relational Database Service (N. Virginia)","summary":"[RESOLVED] Network connectivity","status":3,"date":1312858261,"service":"RelationalDBService","description":"<div><span class=\"yellowfg\"> 7:57 PM PDT</span>&nbsp;We can verify connectivity issues between Amazon RDS DB Instances in the US-EAST-1 region and the internet.  </div><div><span class=\"yellowfg\"> 8:08 PM PDT</span>&nbsp;Full connectivity has been restored. The service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"Connectivity issues","status":2,"date":1312881235,"service":"EC2EU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n\r\n<div><span class=\"yellowfg\">Aug 8 9:58 AM PDT</span>&nbsp;We have added capacity to EBS and are continuing to make progress in recovering EBS volumes and EC2 instances.  We have now also enabled the ability to launch new instance-store backed EC2 instances in the impacted Availability Zone.  Customers can still not create new EBS volumes or launch new EBS backed instances in the affected zone, but we anticipate enabling that soon.  All functionality remains available in the other Availability Zones.</div><div><span class=\"yellowfg\">Aug 8 12:32 PM PDT</span>&nbsp;We have now enabled the ability to create new EBS backed instances and EBS volumes in the affected zone.  API functionality is now fully restored to the affected Availability Zone.  We remain focused on recovering the remaining instances and volumes and are continuing to make progress.</div><div><span class=\"yellowfg\">Aug 8 3:11 PM PDT</span>&nbsp;Separately, and independent from the power issue in the affected availability zone, we've discovered an error in the EBS software that cleans up unused snapshots.  During a recent run of this EBS software in the EU-West Region, one or more blocks in a number of EBS snapshots were incorrectly deleted. The root cause was a software error that caused the snapshot references to a subset of blocks to be missed during the reference counting process. This process compares the blocks scheduled for deletion to the blocks referenced in customer snapshots. As a result of the software error, the EBS snapshot management system in the EU-West Region incorrectly thought some of the blocks were no longer being used and deleted them.  We've addressed the error in the EBS snapshot system to prevent it from recurring.  We have now also disabled all of the snapshots that contain these missing blocks.  \r\n<br><br>\r\nWe are in the process of creating a copy of the affected snapshots where we've replaced the missing blocks with empty block(s). Customers can then create a volume from that copy and run a recovery tool on it (e.g. a file system recovery tool like fsck); in some cases this may restore normal volume operation.  We will email affected customers as soon as we have the copy of their snapshot available. You can tell if you have a snapshot that has been affected via the DescribeSnapshots API or via the AWS Management Console.  The status for the snapshot will be shown as \"error.\"  Alternately, if you have any older or more recent snapshots that were unaffected, you will be able to create a volume from those snapshots without error.  We apologize for any potential impact it might have on customers applications.</div><div><span class=\"yellowfg\">Aug 8 4:26 PM PDT</span>&nbsp;The process of creating copies of data from the affected snapshots is now complete.  These snapshots can be identified via the Description field which you can see on the AWS console or via a DescribeSnapshot API call.  The Description field contains \"Recovery Snapshot snap-xxxx\" where snap-xxx is the id of the affected snapshot.  Customers can create a volume from that copy and run a recovery tool on it (e.g. a file system recovery tool like fsck); in some cases this may restore normal volume operation.  Alternately, if you have any older or more recent snapshots that were unaffected, you will be able to create a volume from those snapshots without error.</div><div><span class=\"yellowfg\">Aug 8 10:01 PM PDT</span>&nbsp;We have now recovered all of the volumes and instances that we could verify were fully consistent at the time of the power outage.  For the remaining EBS volumes, we were unable to verify whether or not there were any in-flight writes that did not get consistently saved.   As a result, we've now started the process of creating recovery snapshots for all of these EBS volumes that are still unavailable.   As these recovery snapshots become available, we will put them in your account.  This process is time consuming and we expect these recovery snapshots to start to show up in customers' accounts in the next 6-8 hours, but the process might take up to 24 hours to fully complete.  We expect that a large portion of these volumes created from these recovery snapshots will be consistent, but customers will need to verify volume consistency by running a recovery tool on their new volume (e.g. a file system recovery tool like fsck).  Volumes that are consistent should be usable by all applications.  If your volume is inconsistent, your application's ability to use the volume will depend on how it handles the inconsistency.  When we make meaningful progress on delivering recovery snapshots to affected customers, we will post an update here.</div><div><span class=\"yellowfg\"> 2:53 AM PDT</span>&nbsp;We are continuing to make steady progress in delivering recovery snapshots to affected customers accounts.  We will continue to post updates here.</div><div><span class=\"yellowfg\"> 8:06 AM PDT</span>&nbsp;We have now delivered recovery snapshots for over half of the volumes that were in an inconsistent state as a result of the power outage.  We are continuing to make steady progress on creation and delivery of the remaining recovery snapshots.</div><div><span class=\"yellowfg\"> 9:53 AM PDT</span>&nbsp;Customers who have an EBS volume in an inconsistent state, and who have received a recovery snapshot, can create a new EBS volume from the recovery snapshot.  But, as of right now an EBS volume that is in this inconsistent state cannot be detached, force detached or snapshotted.  Calling CreateSnapshot for a volume in this state will result in an Internal Error.  Calls to DetachVolume will not complete successfully and the volume will appear stuck in detaching. \r\n\r\nThe AWS Console and the DescribeVolumes API still indicates that inconsistent volumes are \"available\" or \"in-use.\"  This is confusing for customers, but we dont currently have a good way to indicate for each customer which of their volumes are in an inconsistent state.  We're working on being able to provide clearer indication which customers volumes are in an inconsistent state and will reflect that in the AWS Console and in the DescribeVolumes API in the next few hours.  When that happens, we will change the status of the inconsistent volume to \"error.\"  Once that occurs, you will be also able to detach the volume from your instance.</div><div><span class=\"yellowfg\"> 4:34 PM PDT</span>&nbsp;At this point, all impacted volumes in the affected zone that may have had inconsistent writes at the time of the power outage have been moved to status \"error.\"  You can view this status on the AWS console as well as via a DescribeVolumes API call.  If your volume is not in state \"error,\" it remains healthy and available for use.\r\n<br><br>\r\nAdditionally, we continue to make progress in creating recovery snapshots for customers.  These snapshots can be identified via the Description field which you can see on the AWS console or via a DescribeSnapshot API call. The Description field contains \"Recovery Snapshot vol-xxxx\" where vol-xxxx is the id of the impacted volume. Customers can create a new volume using this recovery snapshot and can then run a recovery tool on the new volume (e.g. a file system recovery tool like fsck); in many cases this will restore normal volume operation.</div>","details":""},{"service_name":"Amazon Relational Database Service (Ireland)","summary":"Connectivity issues to RDS instances","status":2,"date":1312881521,"service":"RelationalDBServiceEU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n<div><span class=\"yellowfg\">Aug 8 10:11 PM PDT</span>&nbsp;We have now restored access to all of the single-AZ database instances with storage volumes that we can verify were fully consistent at the time of the power outage.  The small portion of remaining single-AZ databases that are still not available have storage volumes that are inconsistent and will be moved into the \"failed storage\" state.   Our records show that the majority of impacted customers have already initiated Point-in-Time Restore operations to resume database requests.  However, if your database instance is in the \"failed storage\" state and you have not already initiated a Point-in-Time-Restore from an automated backup, we encourage you to do so immediately via these instructions:\r\n<br><br>\r\n1) Log into the AWS Management console<br>\r\n2) Access the RDS tab, and select \"DB Instances\" on the left-side navigation<br>\r\n3) Select the affected database instance<br>\r\n4) Click on the \"Restore to Point in Time\" button<br>\r\n5) Select \"Use Latest Restorable Time\"<br>\r\n6) Select a DB instance class that is at least the same size as the original DB instance<br>\r\n7) Make sure \"No Preference\" is selected for Availability Zone<br>\r\n<br><br>\r\nAdditionally, we would like to update customers on the RDS impact of the EBS snapshot issue described in an earlier EC2 status update.  A portion of user-initiated DB snapshots in the EU-West Region were impacted by the deletion of EBS snapshot blocks.   We are in the process of putting these snapshots into a \"failed\" state.   They will then be identifiable via the DescribeSnapshots API or AWS Management Console, and affected customers will be notified via email.  While these snapshots cannot be recovered, if you have any older or more recent snapshots that were unaffected, you will be able to restore from those snapshots without error.  Please also note that the \"failed\" state only applies to affected user-initiated DB snapshots.  Customers with automated backups turned on should be able to complete Point-in-time-Restore operations as normal.  \r\n<br><br>\r\nWe apologize for any potential impact this issue may have on our customers' applications.</div><div><span class=\"yellowfg\"> 8:45 PM PDT</span>&nbsp;Amazon RDS is operating normally in the EU-West Region.  All Multi-AZ instances were successfully failed over and recovered.   Single-AZ database instances or DB snapshots that could not be recovered have been moved to failed state and affected customers have been notified via email.  The majority of customers with affected Single-AZ database instances had automated backups turned on and have since completed Point-in-Time Restores.  We apologize for any impact on customers applications and businesses.  A more detailed postmortem will be forthcoming.</div>","details":""},{"service_name":"Amazon CloudWatch (Ireland)","summary":"[RESOLVED] Delayed CloudWatch metrics and alarms for EU-WEST-1","status":1,"date":1312881983,"service":"CloudWatchEUWest","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n<div><span class=\"yellowfg\">Aug 7 4:03 PM PDT</span>&nbsp;CloudWatch metrics are operating normally for all recovered instances in the affected Availability Zone. Metrics from instances still recovering from the power loss in the affected Availability Zone are still delayed.</div><div><span class=\"yellowfg\">10:28 AM PDT</span>&nbsp;CloudWatch metrics and alarms are operating normally for the EU-WEST-1 region.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"Connectivity issues","status":2,"date":1312945080,"service":"EC2EU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n\r\n<div><span class=\"yellowfg\">Aug 10 12:30 AM PDT</span>&nbsp;We continue to make regular progress on creating recovery snapshots for customers.   Recovery snapshots can be identified by looking at the Description field which will contain \"Recovery Snapshot vol-xxxx,\" where vol-xxxx is the id of the impact volume.  This process requires us to move and process large amounts of data, which is why it is taking a long time to complete, particularly for some of the larger volumes.  As recovery snapshots become available, customers will see them appear in their accounts.\r\n<br><br>\r\nAdditionally we have discovered that not all of the volumes that we identified as potentially having inconsistent writes are correctly displaying the \"error\" state in API calls or in the AWS Management Console. Volumes in \"error\" that are still attached to instances are still displaying their state as \"in-use.\" Customers can detach those volumes, and the error state will display correctly. We are actively working on a fix to always display the correct state.\r\n<br><br>\r\nVolumes that have been detached and are in an \"error\" state can be deleted, however they may remain in \"deleting\" for an extended period of time. Customers will not be billed for any resources that are either in an \"error\" or \"deleting\" state.\r\n<br><br>\r\nIf a recovery snapshot has been added to your account referring either to \"Recovery snapshot for snap-xxxx\" or \"Recovery snapshot for vol-xxxx\" in the snapshot Description field, the recovery efforts for those volumes or snapshots have been completed. You can create a new volume from these snapshots, and should run a recovery tool against them (e.g. a file system recovery tool like fsck).</div><div><span class=\"yellowfg\">Aug 10,  3:59 AM PDT</span>&nbsp;The API issue we previously identified has been corrected and all of the volumes that we identified as potentially having inconsistent writes are now correctly displaying the \"error\" state in DescribeVolumes API calls or in the AWS Management Console.</div><div><span class=\"yellowfg\">Aug 11,  1:10 AM PDT</span>&nbsp;The vast majority of recovery snapshots have now been created and are in customer accounts.  Recovery snapshots can be identified by looking at the Description field which will contain \"Recovery Snapshot vol-xxxx,\" where vol-xxxx is the id of the impacted volume.  If you have a snapshot or volume that is in \"error\" state, you should have a recovery snapshot for it and can delete the volumes and snapshots that are described as \"error.\"  Less than 2% of the affected volumes are still in the process of having their recovery snapshot created.  You can tell if your volume is part of the remaining 2% being recovered if it is in the \"error\" state and you do not yet see a recovery snapshot.  We are continuing to work to create recovery snapshots for these remaining few volumes.  </div>","details":""},{"service_name":"Amazon Relational Database Service (Ireland)","summary":"[RESOLVED] Connectivity issues to RDS instances","status":1,"date":1313005649,"service":"RelationalDBServiceEU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n<div><span class=\"yellowfg\">Aug 8 10:11 PM PDT</span>&nbsp;We have now restored access to all of the single-AZ database instances with storage volumes that we can verify were fully consistent at the time of the power outage.  The small portion of remaining single-AZ databases that are still not available have storage volumes that are inconsistent and will be moved into the \"failed storage\" state.   Our records show that the majority of impacted customers have already initiated Point-in-Time Restore operations to resume database requests.  However, if your database instance is in the \"failed storage\" state and you have not already initiated a Point-in-Time-Restore from an automated backup, we encourage you to do so immediately via these instructions:\r\n<br><br>\r\n1) Log into the AWS Management console<br>\r\n2) Access the RDS tab, and select \"DB Instances\" on the left-side navigation<br>\r\n3) Select the affected database instance<br>\r\n4) Click on the \"Restore to Point in Time\" button<br>\r\n5) Select \"Use Latest Restorable Time\"<br>\r\n6) Select a DB instance class that is at least the same size as the original DB instance<br>\r\n7) Make sure \"No Preference\" is selected for Availability Zone<br>\r\n<br><br>\r\nAdditionally, we would like to update customers on the RDS impact of the EBS snapshot issue described in an earlier EC2 status update.  A portion of user-initiated DB snapshots in the EU-West Region were impacted by the deletion of EBS snapshot blocks.   We are in the process of putting these snapshots into a \"failed\" state.   They will then be identifiable via the DescribeSnapshots API or AWS Management Console, and affected customers will be notified via email.  While these snapshots cannot be recovered, if you have any older or more recent snapshots that were unaffected, you will be able to restore from those snapshots without error.  Please also note that the \"failed\" state only applies to affected user-initiated DB snapshots.  Customers with automated backups turned on should be able to complete Point-in-time-Restore operations as normal.  \r\n<br><br>\r\nWe apologize for any potential impact this issue may have on our customers' applications.</div><div><span class=\"yellowfg\"> 8:45 PM PDT</span>&nbsp;Amazon RDS is operating normally in the EU-West Region.  All Multi-AZ instances were successfully failed over and recovered.   Single-AZ database instances or DB snapshots that could not be recovered have been moved to failed state and affected customers have been notified via email.  The majority of customers with affected Single-AZ database instances had automated backups turned on and have since completed Point-in-Time Restores.  We apologize for any impact on customers applications and businesses.  A more detailed postmortem will be forthcoming.</div><div><span class=\"yellowfg\"> 8:35 PM PDT</span>&nbsp;Amazon RDS continues to operate normally in the EU-West Region.  A more detailed postmortem on the cause of the recent service disruption will be forthcoming.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Investigating connectivity issues","status":1,"date":1313007588,"service":"EC2","description":"<div><span class=\"yellowfg\"> 1:21 PM PDT</span>&nbsp;We are investigating connectivity issues in a single AZ in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 1:35 PM PDT</span>&nbsp;We can confirm elevated API latencies in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 1:45 PM PDT</span>&nbsp;We are experiencing elevated API error rates for the CreateTags, DescribeTags, DeleteTags and DescribeInstances APIs in the US-EAST-1 region.  All other APIs are functioning normally.  Existing instances and volumes are operating normally.</div><div><span class=\"yellowfg\"> 2:13 PM PDT</span>&nbsp;We experienced increased API error rates for all EC2 Tagging related API calls in the US-EAST-1 region from 1:30PM to 1:44PM PDT.  Additionally from 12:37PM to 12:38PM PDT, EBS volumes in a single AZ experienced increased IO latencies.  The service has now fully recovered and is operating normally.  </div>","details":""},{"service_name":"Amazon SimpleDB (N. California)","summary":"[RESOLVED] Increased error rates","status":2,"date":1313009253,"service":"SimpleDBNoCal","description":"<div><span class=\"yellowfg\"> 2:00 PM PDT</span>&nbsp;We are currently experiencing increased error rates (return code 500) returned by SimpleDB. We will post back here with an update.</div><div><span class=\"yellowfg\"> 2:30 PM PDT</span>&nbsp;We are continuing to investigate increased error rates to Put, Delete, consistent Select, and consistent Get requests made to Amazon SimpleDB in the US-WEST-1 region. </div><div><span class=\"yellowfg\"> 3:22 PM PDT</span>&nbsp;Error rates have now recovered. The service is now operating normally. </div>","details":""},{"service_name":"Amazon Simple Notification Service (N. California)","summary":"[RESOLVED] Increased error rates","status":3,"date":1313010030,"service":"SNSUSWest","description":"<div><span class=\"yellowfg\"> 2:04 PM PDT</span>&nbsp;We are currently experiencing increased error rates in the US-WEST-1 region.</div><div><span class=\"yellowfg\"> 2:21 PM PDT</span>&nbsp;We have confirmed that all calls to SNS APIs and outbound Notifications in US-WEST-1 region are failing. We are actively working on addressing the issue.</div><div><span class=\"yellowfg\"> 2:44 PM PDT</span>&nbsp;Customers should now be able to publish to existing topics and messages are being delivered properly. Topic creation and updates are still not working.</div><div><span class=\"yellowfg\"> 3:23 PM PDT</span>&nbsp;Error rates have now recovered and the service is now operating normally. </div>","details":""},{"service_name":"Amazon CloudWatch (N. California)","summary":"[RESOLVED] Delays in New Metric Creation and Alarms","status":1,"date":1313011878,"service":"CloudWatchNoCal","description":"<div><span class=\"yellowfg\"> 2:34 PM PDT</span>&nbsp;We are experiencing delays in creating new metric names used for retrieval by the CloudWatch ListMetrics API. SNS notification for alarms are also delayed. The PutMetricData and GetMetricStatistics APIs are working properly. </div><div><span class=\"yellowfg\"> 2:53 PM PDT</span>&nbsp;SNS actions are now operating normally for CloudWatch alarms. Customers will receive SNS notifications if their CloudWatch alarm triggers are breached. We are still working to restore service for creation of new metric names for retrieval by the ListMetrics API in CloudWatch. </div><div><span class=\"yellowfg\"> 3:19 PM PDT</span>&nbsp;New metric names that were delayed beginning at 1:20 PM PDT are now backfilled so that customers can access them via the CloudWatch ListMetrics API or the AWS console. The CloudWatch service is operating normally.</div>","details":""},{"service_name":"AWS CloudFormation (N. California)","summary":"[RESOLVED] Delays in stack creation","status":1,"date":1313012702,"service":"CloudFormationNoCal","description":"<div><span class=\"yellowfg\"> 2:49 PM PDT</span>&nbsp;We are currently experiencing delays creating stacks in the US-WEST-1 region.</div><div><span class=\"yellowfg\"> 3:25 PM PDT</span>&nbsp;The creation of stacks is no longer delayed.  The service is now operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"Connectivity issues","status":2,"date":1312964010,"service":"EC2EU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n\r\n<div><span class=\"yellowfg\">Aug 10 12:30 AM PDT</span>&nbsp;We continue to make regular progress on creating recovery snapshots for customers.   Recovery snapshots can be identified by looking at the Description field which will contain \"Recovery Snapshot vol-xxxx,\" where vol-xxxx is the id of the impact volume.  This process requires us to move and process large amounts of data, which is why it is taking a long time to complete, particularly for some of the larger volumes.  As recovery snapshots become available, customers will see them appear in their accounts.\r\n<br><br>\r\nAdditionally we have discovered that not all of the volumes that we identified as potentially having inconsistent writes are correctly displaying the \"error\" state in API calls or in the AWS Management Console. Volumes in \"error\" that are still attached to instances are still displaying their state as \"in-use.\" Customers can detach those volumes, and the error state will display correctly. We are actively working on a fix to always display the correct state.\r\n<br><br>\r\nVolumes that have been detached and are in an \"error\" state can be deleted, however they may remain in \"deleting\" for an extended period of time. Customers will not be billed for any resources that are either in an \"error\" or \"deleting\" state.\r\n<br><br>\r\nIf a recovery snapshot has been added to your account referring either to \"Recovery snapshot for snap-xxxx\" or \"Recovery snapshot for vol-xxxx\" in the snapshot Description field, the recovery efforts for those volumes or snapshots have been completed. You can create a new volume from these snapshots, and should run a recovery tool against them (e.g. a file system recovery tool like fsck).</div><div><span class=\"yellowfg\">Aug 10,  3:59 AM PDT</span>&nbsp;The API issue we previously identified has been corrected and all of the volumes that we identified as potentially having inconsistent writes are now correctly displaying the \"error\" state in DescribeVolumes API calls or in the AWS Management Console.</div><div><span class=\"yellowfg\">Aug 11,  1:10 AM PDT</span>&nbsp;The vast majority of recovery snapshots have now been created and are in customer accounts.  Recovery snapshots can be identified by looking at the Description field which will contain \"Recovery Snapshot vol-xxxx,\" where vol-xxxx is the id of the impacted volume.  If you have a snapshot or volume that is in \"error\" state, you should have a recovery snapshot for it and can delete the volumes and snapshots that are described as \"error.\"  Less than 2% of the affected volumes are still in the process of having their recovery snapshot created.  You can tell if your volume is part of the remaining 2% being recovered if it is in the \"error\" state and you do not yet see a recovery snapshot.  We are continuing to work to create recovery snapshots for these remaining few volumes.  </div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"[RESOLVED] Connectivity issues","status":1,"date":1313050736,"service":"EC2EU","description":"<b>Please refer to the Status History below for the prior day's entries</b><br><br>\r\n\r\n<div><span class=\"yellowfg\">Aug 11,  1:10 AM PDT</span>&nbsp;The vast majority of recovery snapshots have now been created and are in customer accounts.  Recovery snapshots can be identified by looking at the Description field which will contain \"Recovery Snapshot vol-xxxx,\" where vol-xxxx is the id of the impacted volume.  If you have a snapshot or volume that is in \"error\" state, you should have a recovery snapshot for it and can delete the volumes and snapshots that are described as \"error.\"  Less than 2% of the affected volumes are still in the process of having their recovery snapshot created.  You can tell if your volume is part of the remaining 2% being recovered if it is in the \"error\" state and you do not yet see a recovery snapshot.  We are continuing to work to create recovery snapshots for these remaining few volumes.  </div><div><span class=\"yellowfg\"> 7:52 PM PDT</span>&nbsp;Amazon EC2 continues to operate normally in the EU-West Region. A more detailed postmortem on the cause of the recent service disruption will be forthcoming.</div>","details":""},{"service_name":"Amazon Simple Email Service (N. Virginia)","summary":"[RESOLVED] Elevated API error rates","status":1,"date":1313083563,"service":"SES","description":"<div><span class=\"yellowfg\">10:31 AM PDT</span>&nbsp;We are currently experiencing elevated API error rates for the GetSendStatistics API. We have identified the issue and errors rates are recovering. Mail delivery and email address verification is not affected. </div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Error rates have now recovered. The service is operating normally. </div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (Ireland)","summary":"Post mortem for EU-WEST service event","status":1,"date":1313219186,"service":"EC2EU","description":"<div><span class=\"yellowfg\">12:09 AM PDT</span>&nbsp;We have posted a full summary and post mortem of the recent EU West service event <a href=\"http://aws.amazon.com/message/2329B7/\">here</a>.</div></div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"Post mortem for recent US-East service event","status":1,"date":1313219705,"service":"EC2","description":"<div><p><span class=\"yellowfg\">12:26 AM PDT</span>&nbsp;We wanted to provide more detail on the internet connectivity event that occurred from 7:25 PM PDT to 7:55 PM PDT on August 8th in our US East Region.  The event affected connectivity between three different Availability Zones and the internet.</p>  \r\n \r\n<p>The issue happened in the networks that connect our Availability Zones to the internet.  All Availability Zones must have network connectivity to the internet and to each other (to enable a customers resources in one Availability Zone to communicate with resources in other Availability Zones).   Our border and Availability Zone networks use standard routing protocols both to isolate themselves from potential failure in other Availability Zones and to assure continued connection to other Availability Zones or the internet in the face of a failure in any portion of our network.  To prevent network issues in one Availability Zone from impacting any of our other Availability Zones, we use a network routing architecture we refer to as north/south.   Northern routers are at the border, facing toward the internet. Southern routers are part of individual Availability Zones.  To prevent Availability Zones from being able to impact each others routes to the internet, we use standard routing protocols to prevent southern routers in one Availability Zone from advertising internet routes to any other southern router in another Availability Zone.  Southern routers are also prohibited from telling northern routers what routes to use.  This causes routes to only propagate from north to south.</p> \r\n\r\n<p>The event began when a southern router inside one of our Availability Zones briefly stopped exchanging route information with all adjacent devices, going into an incommunicative state.  Upon re-establishing its health, the router began advertising an unusable route to other southern routers in other Availability Zones, deviating from its configuration and bypassing the standard protocol restriction on how routes are allowed to flow.  The bad default internet route was picked up and used by the routers in other Availability Zones.  Internet traffic from multiple Availability Zones in US East was immediately not routable out to the internet through the border.   We resolved the problem by removing the router from service.</p>  \r\n\r\nWe immediately identified that there were no human accesses or automated changes applied to that router. We also have hundreds of thousands of hours of operating experience with this particular router software and hardware configuration.  As a result, we had a strong hypothesis that there was an unusual router software failure causing the router to violate the routing protocol.  As might have been expected (given the long successful experience we've had with this configuration), reproducing the software failure was difficult.  Late Wednesday night, working closely with the supplier of this router, we were able to reproduce the behavior and locate the software bug.  It confirmed our hypothesis of protocol violation.  We've developed a mitigation that can both prevent transmittal of a bad internet route and prevent another router from incorporating that route and using it.  We've tested the mitigation thoroughly and are carefully deploying it throughout our network following our normal change and promotion procedures.</p>  \r\n\r\n<p>We apologize for any impact this event may have caused our customers.  We build with the tenet that even the most serious network failure should not impact more than one Availability Zone beyond the extremely short convergence times required by the network routing protocols.  We will continue to work on eliminating any issues that could jeopardize that important tenet.</p></div>","details":""},{"service_name":"Amazon Relational Database Service (Ireland)","summary":"Post mortem for EU-WEST service event","status":1,"date":1313219340,"service":"RelationalDBServiceEU","description":"<div><span class=\"yellowfg\">12:09 AM PDT</span>&nbsp;We have posted a full summary and post mortem of the recent EU West service event <a href=\"http://aws.amazon.com/message/2329B7/\">here</a>.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud (N. Virginia)","summary":"[RESOLVED] VPN Connectivity Issues","status":1,"date":1313414381,"service":"VirtualPrivateCloud","description":"<div><span class=\"yellowfg\"> 6:20 AM PDT</span>&nbsp;Between 5:56AM PDT and 6:03AM PDT we experienced a connectivity issue to one of our redundant VPN Gateways in the US-EAST-1 region. Each VPN Connection is provisioned as a pair of redundant VPN tunnels to provide customers with the ability to configure a VPN tunnel to two VPN Gateway endpoints. The redundant VPN Gateway was not affected.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Small number of instances unavailable in a single availability zone","status":1,"date":1313676506,"service":"EC2","description":"<div><span class=\"yellowfg\"> 7:10 AM PDT</span>&nbsp;We are investigating connectivity issues for a small number of instances in a single availability zone in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 7:42 AM PDT</span>&nbsp;We have confirmed connectivity issues affecting a small number of instances in a single availability zone in the US-EAST-1 region. To find out if you have instances in the affected availability zone log into the AWS Console at https://console.aws.amazon.com, navigate to the EC2 tab and click on the \"EC2 Dashboard\" link at the top of the navigation bar. The affected availability zone will be shown under the Availability Zone Status.\r\n</div><div><span class=\"yellowfg\"> 8:13 AM PDT</span>&nbsp;Between 6:50AM and 8:05AM PDT a small number of instances were unavailable in a single availability zone in the US-EAST-1 region. All affected instances have been recovered and the service is now operating normally.\r\n</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Investigating connectivity issues","status":1,"date":1313799185,"service":"EC2","description":"<div><span class=\"yellowfg\"> 5:14 PM PDT</span>&nbsp;We are investigating connectivity issues in a single Availability zone in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;We can confirm a small number of instances are unavailable in a single Availability Zone in the US-EAST-1. Affected instances are beginning to recover, and we are working to recover the remaining affected instances.</div><div><span class=\"yellowfg\"> 6:45 PM PDT</span>&nbsp;We can confirm a small number of instances are unavailable in a single Availability Zone in the US-EAST-1. We are continuing working to recover the remaining affected instances.</div><div><span class=\"yellowfg\"> 7:50 PM PDT</span>&nbsp;We are continuing working to recover the remaining affected instances in US-EAST-1. We are also investigating increased API error rates and latencies in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 8:10 PM PDT</span>&nbsp;Between 4:40 PM and 7:50 PM PDT a small number of instances were unavailable in a single Availability Zone in the US-EAST-1 region. The majority of affected instances have recovered and we are continuing to recover the remainder of instances. </div><div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;Between 7:34 PM and 7:44 PM PDT we experienced elevated error rates for RunInstances and TerminateInstances. During that period some API calls were throttled and received a \"resource limit exceeded\" response. The service is now operating normally.</div>","details":""},{"service_name":"Amazon Mechanical Turk (Worker)","summary":"[RESOLVED] Increased Error Rates","status":2,"date":1314445080,"service":"MTURKW","description":"<div><span class=\"yellowfg\"> 4:39 AM PDT</span>&nbsp;We are experiencing elevated error rates for a subset of our worker nodes. We are investigating.</div><div><span class=\"yellowfg\"> 5:09 AM PDT</span>&nbsp;We are continuing to investigate the issue.</div><div><span class=\"yellowfg\"> 5:37 AM PDT</span>&nbsp;Error rates for workers are decreasing. We are continuing to monitor the issue.</div><div><span class=\"yellowfg\"> 6:16 AM PDT</span>&nbsp;The system performance has returned to normal levels. We are continuing to monitor the service.</div><div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;Between 4:30 AM and 5:45 AM, some Workers experienced elevated error rates and slow response times due to a queuing backlog on some of our Worker nodes. \r\n\r\nThe system is now operating normally.</div>","details":""}],"current":[]}