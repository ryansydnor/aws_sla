{"archive":[{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Frankfurt Networking Issues","status":1,"date":1246348299,"service":"CloudFront","description":"<div><span class=\"yellowfg\">June 30, 4:10 PM PDT</span>&nbsp;We'd like to summarize this event and share a little more information about the Amazon CloudFront issue this morning.  The networking issues that we reported were the result of multiple hardware failures in the Frankfurt location.  Due to the nature of the hardware failures, re-routing traffic to a better edge location took longer than usual. This led some customer requests (mostly those coming from close to Frankfurt) to fail from approximately 11:50 PM PDT June 29 to 1:00 AM PDT June 30.  After 1:00 AM PDT, error rates for the service returned to normal levels. \r\n<br><br>\r\nAs we always do in events like these, we will use the data collected in this event to improve our operations going forward. Specifically, in this case we will analyze the process we use to re-route traffic, so we can do so as quickly as possible should events like this arise again in the future.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (US)","summary":"[COMPLETED] Amazon EC2 Network Maintenance","status":1,"date":1246409017,"service":"EC2","description":"<div><span class=\"yellowfg\">5:44 PM PDT</span>&nbsp;We will be performing maintenance on parts of the Amazon EC2 network on Thursday, July 2nd starting at 12:00AM PDT. The maintenance is expected to last approximately 3 hours. The maintenance has been planned to have no impact on running Amazon EC2 instances.  We will update this dashboard when the maintenance window begins and ends. We will also update this dashboard if any issues are detected during the maintenance window.</div>\r\n<div><span class=\"yellowfg\">July 2, 12:03 AM PDT</span>&nbsp;The announced maintenance window is beginning.</div>\r\n<div><span class=\"yellowfg\">July 2, 1:30 AM PDT</span>&nbsp;The scheduled maintenance is now complete.</div>","details":""},{"service_name":"Amazon Mechanical Turk (Worker)","summary":"[RESOLVED] Elevated error rates","status":2,"date":1248030066,"service":"MTURKW","description":"<div>We'd like to provide some additional information regarding the Amazon Mechanical Turk event of July 19.  On that day, a power outage in one location in our East coast region impacted two key databases that support the Mechanical Turk web site.  The first database was impacted at 11:16 AM PDT and went offline.   We brought the database back online and recovery started at 11:25 AM PDT.  By 12:16 PM PDT the system was operating normally.  The second database went offline at 12:33 PM PDT.  We brought the database back online and recovery started by 12:50 PM.  By 1:37 PM PDT the system was operating normally.  During both database issues, workers may have experienced elevated error rates, impacting their ability to accept HITs.</div>\r\n<br>\r\n<div>Our system is designed to handle power events.  However, in this case, we experienced a short loss of both primary power and back-up power generator.  To prevent reoccurrences of this type of issue, we are in the process of improving our power redundancy for these critical systems.</div>","details":""},{"service_name":"Amazon Mechanical Turk (Requester)","summary":"[RESOLVED] Elevated error rates","status":2,"date":1248033019,"service":"MTURKR","description":"<div>We'd like to provide some additional information regarding the Amazon Mechanical Turk event of July 19.  On that day, a power outage in one location in our East coast region impacted two key databases that support the Requester APIs.  The first database was impacted at 11:16 AM PDT and went offline.  We brought the database back online and recovery started at 11:25 AM PDT.  By 12:16 PM PDT the Requester APIs were operating normally.  The second database went offline at 12:33 PM PDT.   We brought the database back online and recovery started by 12:50 PM. By 1:37 PM PDT the Requester APIs were operating normally.  During both database issues, requesters may have experienced elevated error rates, impacting their ability to create HITs and approve assignments.</div>\r\n<br>\r\n<div>Our system is designed to handle power events. However, in this case, we experienced a short loss of both primary power and back-up power generator.  To prevent reoccurrences of this type of issue, we are in the process of improving our power redundancy for these critical systems.</div>","details":""},{"service_name":"Amazon SimpleDB","summary":"[RESOLVED] Elevated error rates","status":2,"date":1248033104,"service":"SDB","description":"<div>We'd like to provide some additional information regarding the Amazon SimpleDB event of July 19.  During this event, some Amazon SimpleDB customers experienced elevated error rates.</div>\r\n<br>\r\n<div>One location in our East coast region suffered a short loss of both primary power and back-up generator.  This affected some critical networking equipment used by our service, resulting in elevated error rates from 12:31 PM PDT to 12:57 PM PDT.  Some Amazon SimpleDB customers also experienced higher than normal request latencies during that same time period.  To further reduce the chance of this kind of failure, we are in the process of improving our power redundancy for these critical systems.</div>\r\n","details":""},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Elevated origin error rates","status":1,"date":1248034259,"service":"CloudFront","description":"<div>We'd like to provide some additional information regarding the Amazon CloudFront event of July 19.  On that day, some Amazon CloudFront customers experienced a period of elevated error rates when accessing our control API.  The Amazon CloudFront control API stores configuration information for customers' distributions - information like cloudfront.net DNS Names' CNAMEs and logging configuration settings. Customers call the control API in order to change these settings. The control API is not called when CloudFront delivers content to end users.</div> \r\n<br>\r\n<div>At 12:31 PM PDT, a power outage affected one of the databases used by our control API.  We designed our database to failover to a secondary in the event of an issue such as this. Unfortunately, this automatic failover to the secondary database did not work properly. As a result, from 12:31 PM to 1:25 PM PDT, control API error rates were elevated and the control API became unreachable at 1:25PM PDT. Connectivity to the database was restored at 2:00 PM and the control API started functioning normally. Throughout this event, Amazon CloudFront continued to serve content, and no control API data was lost. </div>\r\n<br>\r\n<div>We're taking two corrective steps as a result of this incident. First, in this case, the failure resulted from a short loss of both our primary power and back-up generator. To further reduce the chance of these kinds of failures, we are in the process of improving our power redundancy for these critical systems. Second, we're scrutinizing and improving the process we use to failover our control API database.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (US)","summary":"[RESOLVED] Elevated packet loss.","status":1,"date":1248034486,"service":"EC2","description":"<div>We'd like to provide some additional information regarding the Amazon EC2 event of July 19.  In our earlier posts, we outlined two periods of elevated packet loss that were experienced by some Amazon EC2 customers.  Each period corresponds to a different root cause.</div>\r\n<br>\r\n<div>The first issue was caused by a loss of power in a single Availability Zone within our East coast region.  While only a limited number of hosts were impacted by this issue, some critical networking equipment in the Availability Zone lost power.  This resulted in a loss of connectivity between some instances and the Internet from 12:31 PM until 12:46 PM PDT.  Once power was restored to the networking equipment, all impacted instances regained connectivity. While other Availability Zones were unaffected by this issue, our Availability Zones are engineered to avoid loss of critical systems when power events happened.  In this case, a failure resulted from a short loss of both our primary power and back-up generator.  To further reduce the chance of this kind of failure, we are in the process of improving our power redundancy (for these critical systems) in this Availability Zone with set-ups that we have successfully deployed in other Availability Zones.</div>\r\n<br>\r\n<div>The second issue was caused by a change made to shift traffic away from the network devices that had lost power.  We decided to proactively shift traffic away from the network devices that failed as an interim plan until we fully understood the cause of the power failure.  Unfortunately, an error in the new routing configuration introduced intermittent packet loss for a subset of traffic in the Region.  Once the bad configuration was isolated, it was corrected, and full connectivity was restored to all instances.  To prevent recurrences of this type of error, we are modifying our routing topology to simplify our traffic shifting procedure.\r\n</div>","details":""},{"service_name":"Amazon Elastic MapReduce","summary":"[RESOLVED] Elevated error rates","status":1,"date":1248035036,"service":"ElasticMapReduce","description":"<div><span class=\"yellowfg\">1:25 PM PDT</span>&nbsp;We are investigating elevated error rates. </div>\n<div><span class=\"yellowfg\">2:22 PM PDT</span>&nbsp;Error rates are recovering. We are continuing to monitor the service.</div>\n<div><span class=\"yellowfg\">2:41 PM PDT</span>&nbsp;Error rates have recovered. The service is operating normally.</div>\n","details":""},{"service_name":"Amazon Simple Queue Service","summary":"[RESOLVED] Connectivity Issues","status":1,"date":1248044542,"service":"SQS","description":"<div>We'd like to provide some additional information regarding the Amazon SQS event of July 19th.  A power outage resulted in some of our networking equipment in one of our datacenters being impacted. For the period between 12:31 PM - 12:52 PM PDT, a portion of connection attempts timed out and some experienced elevated error rates. However, queues continued to be available for clients who had retry logic built into their applications. The net result of this was our setting SQS to Green-i status.</div>\r\n<br>\r\n<div>At 12:52 PM PDT, we updated our DNS settings, which resulted in almost all clients being redirected to healthy datacenters. At 1:05 PM PDT, the networking issues in the one affected datacenter were fully resolved and errors on connection attempts to Amazon SQS ended.</div>","details":""}],"current":[]}