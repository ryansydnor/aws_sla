{"archive":[{"service_name":"Amazon Simple Storage Service (US Standard)","summary":"[RESOLVED] Elevated error rates for COPY operations.","status":1,"date":1258397243,"service":"S3US","description":"<div><span class=\"yellowfg\">10:48 AM PST</span>&nbsp;We are currently experiencing elevated error rates for COPY operations. All other operations are working normally.</div><div><span class=\"yellowfg\">11:17 AM PST</span>&nbsp;COPY operations taking over 5 seconds to complete are returning a 500 Internal Server Error. We have identified the cause of this issue and are actively working on a fix.</div><div><span class=\"yellowfg\">11:57 AM PST</span>&nbsp;This issue has been resolved. The service is now operating normally.</div>","details":""},{"service_name":"Amazon Simple Storage Service (EU - Ireland)","summary":"[RESOLVED] Elevated error rates for COPY operations.","status":1,"date":1258397314,"service":"S3EU","description":"<div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We are currently experiencing elevated error rates for COPY operations. All other operations are working normally.</div><div><span class=\"yellowfg\">11:16 AM PST</span>&nbsp;COPY operations taking over 5 seconds to complete are returning a 500 Internal Server Error. We have identified the cause of this issue and are actively working on a fix.</div><div><span class=\"yellowfg\">11:36 AM PST</span>&nbsp;This issue has been resolved. The service is now operating normally.</div>","details":""},{"service_name":"Amazon Flexible Payments Service","summary":"[COMPLETED] Amazon FPS Maintenance","status":1,"date":1258575633,"service":"FPS","description":"<div><div><span class=\"yellowfg\">Nov 18 12:22 PM PST</span>&nbsp;Amazon FPS will be undergoing scheduled maintenance on Nov 19, from 1:00 AM to 2:30 AM PST. During this maintenance window there will be a short period of time when requests to Pay (CC and ACH), Refund, Settle, Reserve, CreateAccount, and ASP Subscriptions will result in a 500 Internal Server Error. The Get* API calls, (GetAccountBalance, GetAccountActivity, etc.), will operate normally. We will update this dashboard when the maintenance window begins and ends. We will also update this dashboard if any issues are detected during the maintenance window.</div><div><span class=\"yellowfg\">Nov 19 1:00 AM PST</span>&nbsp;The announced maintenance window is beginning.</div><div><span class=\"yellowfg\">Nov 19 2:03 AM PST</span>&nbsp;The scheduled maintenance is now complete.</div></div>","details":""},{"service_name":"Amazon Virtual Private Cloud","summary":"[RESOLVED] 32-bit instance boot failures","status":1,"date":1258577337,"service":"VirtualPrivateCloud","description":"<div><span class=\"yellowfg\"> 1:21 PM PST</span>&nbsp;We are investigating boot failures for newly launched 32-bit instance types in some Amazon Virtual Private Clouds (Amazon VPC). All 64-bit instance types are booting normally. Currently running instances are not affected.\r\n</div><div><span class=\"yellowfg\"> 2:39 PM PST</span>&nbsp;Just a quick update that we are continuing to work on this issue. Customers that need to launch new instances in Amazon VPC should continue to use 64-bit instance types.</div><div><span class=\"yellowfg\"> 3:49 PM PST</span>&nbsp;32-bit instances are now booting successfully.  The service is operating normally.</div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (US - N. Virginia)","summary":"[RESOLVED] Some delays in Amazon CloudWatch metrics","status":1,"date":1258658787,"service":"EC2","description":"<div><span class=\"yellowfg\">11:43 AM PST</span>&nbsp;We are investigating delays for some Amazon CloudWatch metrics in the US-EAST-1 region.</div><div><span class=\"yellowfg\">12:07 PM PST</span>&nbsp;We have corrected the delay in metrics.  The service is now operating normally.</div>","details":""},{"service_name":"AWS Management Console","summary":"[RESOLVED] Intermittent Login Errors","status":1,"date":1258878497,"service":"ManagementConsole","description":"<div><span class=\"yellowfg\">12:31 AM PST</span>&nbsp;Customers may be experiencing intermittent problems logging into the AWS Management Console.  We are investigating.</div><div><span class=\"yellowfg\">12:50 AM PST</span>&nbsp;The problem has been resolved.  Console login operations  have returned to normal.</div>","details":""},{"service_name":"Amazon Mechanical Turk (Worker)","summary":"[RESOLVED] High Latencies on the Worker site","status":1,"date":1259155401,"service":"MTURKW","description":"<div><span class=\"yellowfg\"> 5:30 AM PST</span>&nbsp;We are currently investigating increased page load times on the Amazon Mechanical Turk Worker site.</div><div><span class=\"yellowfg\"> 6:03 AM PST</span>&nbsp;Latencies are recovering and we are continuing to monitor the service.</div><div><span class=\"yellowfg\"> 6:45 AM PST</span>&nbsp;Latencies have returned to expected levels. The service is now operating normally.</div>","details":""},{"service_name":"Amazon Virtual Private Cloud","summary":"[RESOLVED] Instance launch failures","status":1,"date":1259156322,"service":"VirtualPrivateCloud","description":"<div><span class=\"yellowfg\"> 5:46 AM PST</span>&nbsp;We are investigating elevated launch failure rates affecting some Virtual Private Clouds (VPCs).</div><div><span class=\"yellowfg\"> 6:06 AM PST</span>&nbsp;Launch requests have now returned to normal.  At no time were running instances impacted.</div>","details":""},{"service_name":"Amazon Mechanical Turk (Requester)","summary":"[RESOLVED] Elevated error rates","status":2,"date":1260348776,"service":"MTURKR","description":"<div><div><span class=\"yellowfg\">Dec 11  3:05 PM PST</span>&nbsp;We would like to provide further information about the elevated error rates experienced by Requesters between Dec 8th 11:45pm PST and Dec 9th 7:35am PST.  During this period of time the service was impacted by two independent issues.  The first issue was caused by a loss of power to some of the Mechanical Turk databases.  This loss of power was the result of redundant components in a power distribution system failing.  The service recovered from this issue at 12:55am PST after we successfully failed over to a set of backup databases.  The second period of elevated errors started at 3:35am PST and was the result of a recently modified query not performing well.  A fix was deployed to address this issue and the service was fully recovered by 7:35am PST.  During both of these issues, a subset of customers experienced elevated error rates when performing operations on HITs and qualifications.</div></div>","details":""},{"service_name":"Amazon Mechanical Turk (Worker)","summary":"[RESOLVED] Elevated error rates","status":2,"date":1260348923,"service":"MTURKW","description":"<div><div><span class=\"yellowfg\">Dec 11  3:05 PM PST</span>&nbsp;We would like to provide further information about the elevated error rates experienced by Workers between Dec 8th 11:45pm PST and Dec 9th 7:35am PST.  During this period of time the service was impacted by two independent issues.  The first issue was caused by a loss of power to some of the Mechanical Turk databases.  This loss of power was the result of redundant components in a power distribution system failing.  The service recovered from this issue at 12:55am PST after we successfully failed over to a set of backup databases.  The second period of elevated errors started at 3:35am PST and was the result of a recently modified query not performing well.  A fix was deployed to address this issue and the service was fully recovered by 7:35am PST.  During both of these issues, a subset of customers experienced elevated error rates when performing operations on HITs and qualifications.</div></div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (US - N. Virginia)","summary":"[RESOLVED] Power issues in a single availability zone","status":2,"date":1260349672,"service":"EC2","description":"<div><div><span class=\"yellowfg\">Dec 9 1:08 AM PST</span>&nbsp;We are investigating connectivity issues for instances in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 1:26 AM PST</span>&nbsp;We are experiencing power issues for a subset of instances in a single availability zone in the US-EAST-1 region.</div><div><span class=\"yellowfg\"> 1:51 AM PST</span>&nbsp;The underlying power issue has been addressed. Instances have begun to recover.</div><div><span class=\"yellowfg\"> 2:11 AM PST</span>&nbsp;Most affected instances have regained power and are operating normally. We are working to recover a small number of instances that have not yet recovered.</div><div><span class=\"yellowfg\"> 3:33 AM PST</span>&nbsp;We are continuing to work on recovering the remaining affected instances.</div><div><span class=\"yellowfg\"> 5:31 AM PST</span>&nbsp;We are continuing to work on recovering the small number of remaining affected instances.</div><div><span class=\"yellowfg\"> 6:41 AM PST</span>&nbsp;We have completed the recovery of most instances affected by this event. A very small number of instances are still not responding. We will continue to attempt to restore all instances or notify users directly if the instances remain degraded.</div><br/><div><span class=\"yellowfg\">Dec 15 12:08 PM PST</span>&nbsp;(revision of Dec 10 post to clarify timing)  We would like to provide further information on the issue experienced on December 9 in one of our east coast availability zones (this event affected a minority of instances in one of our four Availability Zones in the US-EAST-1 Region).  A single component of the redundant power distribution system failed in this zone. Prior to completing the repair of this unit, a second component, used to assure redundant power paths, failed as well, resulting in a portion of the servers in that availability zone losing power. Impacted customers experienced a loss of connectivity to their instances. As soon as the defective power distribution units were bypassed, servers restarted and instances began to come online shortly thereafter.  Over 25% of these instances recovered within 30 minutes, and over 90% recovered within an hour. A small number of instances took up to a few hours to recover, and we worked with those customers during the morning.</div></div>","details":""},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] CloudFront Control API error rate","status":1,"date":1260352307,"service":"CloudFront","description":"<div><span class=\"yellowfg\"> 1:54 AM PST</span>&nbsp;We're investigating elevated error rates to our control API. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=\"yellowfg\"> 2:04 AM PST</span>&nbsp;Calls to the CloudFront Control APIs have returned to normal. At no time were end-user requests impacted.</div>","details":""},{"service_name":"Amazon SimpleDB (US - N. California)","summary":"[COMPLETE] Amazon SimpleDB Maintenance","status":1,"date":1260934508,"service":"SimpleDBNoCal","description":"<div><span class=\"yellowfg\"> 7:37 PM PST</span>&nbsp;Beginning on Dec 16th at 10:00 AM PST, domain operations in the US-WEST-1 Region will be disabled for a period of 4 hours.  Any CreateDomain or DeleteDomain API calls made during this time period will result in a 503 response.  While we undertake this scheduled maintenance, all other API calls will continue to be served.</div><div><span class=\"yellowfg\">Dec 16, 10:02 AM PST</span>&nbsp;The maintenance window is now beginning.</div><div><span class=\"yellowfg\">Dec 16,  2:07 PM PST</span>&nbsp;Maintenance is proceeding and expected to complete within the hour.</div><div><span class=\"yellowfg\">Dec 16,  3:02 PM PST</span>&nbsp;Maintenance is complete. All API calls are available.</div>","details":""},{"service_name":"Amazon Simple Storage Service (US - N. California)","summary":"[RESOLVED] Increased Error Rates","status":2,"date":1261111540,"service":"S3NoCal","description":"<div><div><span class=\"yellowfg\">Dec 17, 8:48 PM PST</span>&nbsp;We're investigating increased errors.</div><div><span class=\"yellowfg\">Dec 17, 9:02 PM PST</span>&nbsp;A large percentage of authenticated requests are failing.  Anonymous requests are succeeding.\r\n</div><div><span class=\"yellowfg\">Dec 17, 9:27 PM PST</span>&nbsp;Error rates are significantly reduced.\r\n</div><div><span class=\"yellowfg\">Dec 17, 9:34 PM PST</span>&nbsp;All APIs are functioning normally.</div><div><span class=\"yellowfg\">Dec 18, 12:50 AM PST</span>&nbsp;To follow up, at Dec 17 5PM PST, we began running a set of Amazon SimpleDB production tests in our recently launched Northern California region.  We do these tests to assure our production systems continually meet specification.  Among the calls being exercised was an infrequently utilized authentication operation.  The service which authenticates AWS requests functioned normally until 8:15PM PST at which time the fleet of servers handling these requests began rapidly locking up.  We identified the issue, terminated the test, and restarted the fleet of servers handling this kind of request.  The restart was completed at 9:20PM PST and authenticated request functions returned to normal. While the rates at which we were making this request were far beyond anything we ever expect to see, we do normally handle these test rates in other regions.  We've adjusted the capacity for this function and are isolating and fixing the source of the lockup to prevent this from being an issue at these high loads.  During this issue, Amazon EC2 instances, Amazon EBS volumes, Amazon S3 anonymous gets, and Amazon CloudFront content delivery continued to function uninterrupted.</div></div>","details":""},{"service_name":"Amazon SimpleDB (US - N. California)","summary":"[RESOLVED] Increased Error Rates","status":3,"date":1261111696,"service":"SimpleDBNoCal","description":"<div><div><span class=\"yellowfg\">Dec 17, 8:49 PM PST</span>&nbsp;We're investigating increased errors.</div><div><span class=\"yellowfg\">Dec 17, 9:03 PM PST</span>&nbsp;A large percentage of authenticated requests are failing.\r\n</div><div><span class=\"yellowfg\">Dec 17, 9:31 PM PST</span>&nbsp;All APIs are functioning normally.</div><div><span class=\"yellowfg\">Dec 18, 12:54 AM PST</span>&nbsp;To follow up, at Dec 17 5PM PST, we began running a set of Amazon SimpleDB production tests in our recently launched Northern California region.  We do these tests to assure our production systems continually meet specification.  Among the calls being exercised was an infrequently utilized authentication operation.  The service which authenticates AWS requests functioned normally until 8:15PM PST at which time the fleet of servers handling these requests began rapidly locking up.  We identified the issue, terminated the test, and restarted the fleet of servers handling this kind of request.  The restart was completed at 9:20PM PST and authenticated request functions returned to normal. While the rates at which we were making this request were far beyond anything we ever expect to see, we do normally handle these test rates in other regions.  We've adjusted the capacity for this function and are isolating and fixing the source of the lockup to prevent this from being an issue at these high loads.  During this issue, Amazon EC2 instances, Amazon EBS volumes, Amazon S3 anonymous gets, and Amazon CloudFront content delivery continued to function uninterrupted.</div></div>","details":""},{"service_name":"Amazon Elastic Compute Cloud (US - N. California)","summary":"[RESOLVED] Increased Error Rates","status":2,"date":1261111812,"service":"EC2NoCal","description":"<div><div><span class=\"yellowfg\">Dec 17, 8:51 PM PST</span>&nbsp;We're investigating increased errors.</div><div><span class=\"yellowfg\">Dec 17, 9:06 PM PST</span>&nbsp;A large percentage of authenticated requests are failing.  All running instances and volumes are available and are operating normally.</div><div><span class=\"yellowfg\">Dec 17, 9:25 PM PST</span>&nbsp;All APIs are functioning normally.</div><div><span class=\"yellowfg\">Dec 18, 12:47AM PST</span>&nbsp;To follow up, at Dec 17 5PM PST, we began running a set of Amazon SimpleDB production tests in our recently launched Northern California region.  We do these tests to assure our production systems continually meet specification.  Among the calls being exercised was an infrequently utilized authentication operation.  The service which authenticates AWS requests functioned normally until 8:15PM PST at which time the fleet of servers handling these requests began rapidly locking up.  We identified the issue, terminated the test, and restarted the fleet of servers handling this kind of request.  The restart was completed at 9:20PM PST and authenticated request functions returned to normal. While the rates at which we were making this request were far beyond anything we ever expect to see, we do normally handle these test rates in other regions.  We've adjusted the capacity for this function and are isolating and fixing the source of the lockup to prevent this from being an issue at these high loads.  During this issue, Amazon EC2 instances, Amazon EBS volumes, Amazon S3 anonymous gets, and Amazon CloudFront content delivery continued to function uninterrupted.</div></div>","details":""},{"service_name":"Amazon Simple Queue Service (US - N. California)","summary":"[RESOLVED] Increased Error Rates","status":3,"date":1261112272,"service":"SQSNoCal","description":"<div><div><span class=\"yellowfg\">Dec 17, 8:58 PM PST</span>&nbsp;We're investigating increased errors.</div><div><span class=\"yellowfg\">Dec 17, 9:07 PM PST</span>&nbsp;A large percentage of authenticated requests are failing. </div><div><span class=\"yellowfg\">Dec 17, 9:32 PM PST</span>&nbsp;All APIs are functioning normally.</div><div><span class=\"yellowfg\">Dec 18, 12:52 AM PST</span>&nbsp;To follow up, at Dec 17 5PM PST, we began running a set of Amazon SimpleDB production tests in our recently launched Northern California region.  We do these tests to assure our production systems continually meet specification.  Among the calls being exercised was an infrequently utilized authentication operation.  The service which authenticates AWS requests functioned normally until 8:15PM PST at which time the fleet of servers handling these requests began rapidly locking up.  We identified the issue, terminated the test, and restarted the fleet of servers handling this kind of request.  The restart was completed at 9:20PM PST and authenticated request functions returned to normal. While the rates at which we were making this request were far beyond anything we ever expect to see, we do normally handle these test rates in other regions.  We've adjusted the capacity for this function and are isolating and fixing the source of the lockup to prevent this from being an issue at these high loads.  During this issue, Amazon EC2 instances, Amazon EBS volumes, Amazon S3 anonymous gets, and Amazon CloudFront content delivery continued to function uninterrupted.</div></div>","details":""}],"current":[]}